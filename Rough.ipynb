{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssawmya-local/.conda/envs/SageMix/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from SageMix import SageMix\n",
    "from data import ModelNet40, ScanObjectNN\n",
    "from model import PointNet, DGCNN\n",
    "from util import cal_loss, cal_loss_mix, IOStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(batch_size=8, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.0001, model='pointnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1024\n",
    "dataset = ModelNet40(partition='train', num_points=num_points)\n",
    "batch_size=args.batch_size\n",
    "# print(dataset)\n",
    "# dataset = dataset[:100]\n",
    "# batch_size = len(dataset)\n",
    "# print('args.batch_size:',batch_size)\n",
    "test_batch_size = args.test_batch_size\n",
    "train_loader = DataLoader(dataset, num_workers=8,\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', num_points=num_points), num_workers=8,\n",
    "                        batch_size=test_batch_size, shuffle=True, drop_last=False)\n",
    "num_class=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emd_ import emd_module\n",
    "\n",
    "class SageMix:\n",
    "    def __init__(self, args, device, num_class=40):\n",
    "        self.num_class = num_class\n",
    "        self.EMD = emd_module.emdModule()\n",
    "        self.sigma = args.sigma\n",
    "        self.beta = torch.distributions.beta.Beta(torch.tensor([args.theta]), torch.tensor([args.theta]))\n",
    "        self.device = device\n",
    "\n",
    "    def mix(self, xyz, label, saliency=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xyz (B,N,3)\n",
    "            label (B)\n",
    "            saliency (B,N): Defaults to None.\n",
    "        \"\"\"        \n",
    "        B, N, _ = xyz.shape\n",
    "        # print(xyz.shape)\n",
    "        idxs = torch.randperm(B)\n",
    "\n",
    "        \n",
    "        #Optimal assignment in Eq.(3)\n",
    "        # perm = xyz[idxs]\n",
    "        dist_mat = torch.empty(B, B, 1024)\n",
    "        ass_mat = torch.empty(B,B,1024)\n",
    "        dist_mat = dist_mat.to(self.device)\n",
    "        \n",
    "        # print(\"Starting to compute optimal assignment (Heuristic-1)\")\n",
    "        for idx,point in enumerate(xyz):\n",
    "            # perm = torch.tensor([point for x in range(B))\n",
    "            # print(point.shape)\n",
    "            perm = point.repeat(B,1)\n",
    "            # print(perm.shape)\n",
    "\n",
    "            perm  = perm.reshape(perm.shape[0]//1024,1024,3)\n",
    "            \n",
    "            dist, ass = self.EMD(xyz, perm, 0.005, 500) # mapping\n",
    "                 # 32,1024\n",
    "            dist_mat[idx] = dist\n",
    "            ass_mat[idx] = ass\n",
    "\n",
    "            # print('dist:',dist.shape)\n",
    "            # if idx % 10 == 0:\n",
    "            #     print(\"Now doing\", idx)\n",
    "        \n",
    "        # print(dist_mat.shape)\n",
    "        dist_mat = torch.norm(dist_mat,dim=2)\n",
    "        dist_mat = dist_mat.fill_diagonal_(100000.0)\n",
    "        \n",
    "        # print(dist_mat)\n",
    "        # print(dist_mat.shape)\n",
    "        # print(ass_mat.shape)\n",
    "        # print(dist_mat)\n",
    "        i,j = divmod(torch.argmin(dist_mat).item(),dist_mat.shape[1])\n",
    "        ass = ass_mat[i]\n",
    "        # print(i,j)\n",
    "        # print(dist_mat[i,j])\n",
    "        # argmin = torch.argmin(dist_mat)\n",
    "        # i,j = argmin\n",
    "        ass = ass.long()\n",
    "\n",
    "        # print(\"Done with compute optimal assignment (Heuristic-1)\")\n",
    "        # print(ass.shape)\n",
    "        perm_new = torch.zeros_like(perm).to(self.device)\n",
    "        perm_saliency = torch.zeros_like(saliency).to(self.device)\n",
    "        \n",
    "        # print(ass,ass.shape)\n",
    "        for i in range(B):\n",
    "            perm_new[i] = perm[i][ass[i]]\n",
    "            perm_saliency[i] = saliency[idxs][i][ass[i]]\n",
    "        \n",
    "        #####\n",
    "        # Saliency-guided sequential sampling\n",
    "        #####\n",
    "        #Eq.(4) in the main paper\n",
    "        epsilon = 1e-3\n",
    "        # print('sal0:',saliency.sum(-1, keepdim=True))\n",
    "        sum_sal = saliency.sum(-1, keepdim=True) + epsilon\n",
    "        # print('sum_sal:',sum_sal)\n",
    "        saliency = saliency/sum_sal\n",
    "        # print('sal1:',saliency)\n",
    "        anc_idx = torch.multinomial(saliency, 1, replacement=True)\n",
    "        anchor_ori = xyz[torch.arange(B), anc_idx[:,0]]\n",
    "        \n",
    "        #cal distance and reweighting saliency map for Eq.(5) in the main paper\n",
    "        sub = perm_new - anchor_ori[:,None,:]\n",
    "        dist = ((sub) ** 2).sum(2).sqrt()\n",
    "        perm_saliency = perm_saliency * dist\n",
    "        perm_saliency = perm_saliency/perm_saliency.sum(-1, keepdim=True)\n",
    "        \n",
    "        #Eq.(5) in the main paper\n",
    "        anc_idx2 = torch.multinomial(perm_saliency, 1, replacement=True)\n",
    "        anchor_perm = perm_new[torch.arange(B),anc_idx2[:,0]]\n",
    "                \n",
    "                \n",
    "        #####\n",
    "        # Shape-preserving continuous Mixup\n",
    "        #####\n",
    "        alpha = self.beta.sample((B,)).cuda()\n",
    "        sub_ori = xyz - anchor_ori[:,None,:]\n",
    "        sub_ori = ((sub_ori) ** 2).sum(2).sqrt()\n",
    "        #Eq.(6) for first sample\n",
    "        ker_weight_ori = torch.exp(-0.5 * (sub_ori ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "        \n",
    "        sub_perm = perm_new - anchor_perm[:,None,:]\n",
    "        sub_perm = ((sub_perm) ** 2).sum(2).sqrt()\n",
    "        #Eq.(6) for second sample\n",
    "        ker_weight_perm = torch.exp(-0.5 * (sub_perm ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "        \n",
    "        #Eq.(9)\n",
    "        weight_ori = ker_weight_ori * alpha \n",
    "        weight_perm = ker_weight_perm * (1-alpha)\n",
    "        weight = (torch.cat([weight_ori[...,None],weight_perm[...,None]],-1)) + 1e-16\n",
    "        weight = weight/weight.sum(-1)[...,None]\n",
    "\n",
    "        #Eq.(8) for new sample\n",
    "        x = weight[:,:,0:1] * xyz + weight[:,:,1:] * perm_new\n",
    "        \n",
    "        #Eq.(8) for new label\n",
    "        target = weight.sum(1)\n",
    "        target = target / target.sum(-1, keepdim=True)\n",
    "        label_onehot = torch.zeros(B, self.num_class).to(self.device).scatter(1, label.view(-1, 1), 1)\n",
    "        label_perm_onehot = label_onehot[idxs]\n",
    "        label = target[:, 0, None] * label_onehot + target[:, 1, None] * label_perm_onehot \n",
    "        \n",
    "        return x, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 6 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1230 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = PointNet(args, num_class).to(device)\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "    \n",
    "\n",
    "sagemix = SageMix(args, device, num_class)\n",
    "criterion = cal_loss_mix\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    ####################\n",
    "    # Train\n",
    "    ####################\n",
    "    train_loss = 0.0\n",
    "    count = 0.0\n",
    "    model.train()\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        batch_size = data.size()[0]\n",
    "        \n",
    "        ####################\n",
    "        # generate augmented sample\n",
    "        ####################\n",
    "        model.eval()\n",
    "        data_var = Variable(data.permute(0,2,1), requires_grad=True)\n",
    "        logits = model(data_var)\n",
    "        loss = cal_loss(logits, label, smoothing=False)\n",
    "        loss.backward()\n",
    "        opt.zero_grad()\n",
    "        saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "        data, label = sagemix.mix(data, label, saliency)\n",
    "        # break\n",
    "        model.train()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        logits = model(data.permute(0,2,1))\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        count += batch_size\n",
    "        train_loss += loss.item() * batch_size\n",
    "        \n",
    "    scheduler.step()\n",
    "    outstr = 'Train %d, loss: %.6f' % (epoch, train_loss*1.0/count)\n",
    "    print(outstr)\n",
    "    # io.cprint(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e43317eb3113a636e59ebf4e4d52ed79ac7360830f592e9d05ab9479dd90e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
