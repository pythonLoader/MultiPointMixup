{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from SageMix import SageMix\n",
    "from data import ModelNet40, ScanObjectNN\n",
    "from model import PointNet, DGCNN\n",
    "from util import cal_loss, cal_loss_mix, IOStream\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "# import io\n",
    "\n",
    "import torch\n",
    "from emd_ import emd_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageMix:\n",
    "    def __init__(self, args, num_class=40):\n",
    "        self.num_class = num_class\n",
    "        self.EMD = emd_module.emdModule()\n",
    "        self.sigma = args.sigma\n",
    "        self.beta = torch.distributions.beta.Beta(torch.tensor([args.theta]), torch.tensor([args.theta]))\n",
    "    \n",
    "    def mix(self, xyz, label, saliency=None, n_mix=4, theta=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xyz (B,N,3)\n",
    "            label (B)\n",
    "            saliency (B,N): Defaults to None.\n",
    "        \"\"\"        \n",
    "        # label_ori = label.clone()\n",
    "        # print(xyz.shape)\n",
    "        B, N, _ = xyz.shape\n",
    "        # print(\"saliency based\", saliency_based)\n",
    "        # mapping = self.find_optimal_mapping(xyz, saliency)\n",
    "        # return 0\n",
    "        # print(mapping)\n",
    "        # return 0\n",
    "        # print(xyz.shape)\n",
    "        # idxs = mapping[:,1].to(torch.int64) #torch.randperm(B)\n",
    "        idxs = torch.stack([torch.randperm(B) for _ in range(n_mix)])\n",
    "        # idxs = torch.argsort(torch.rand(B, n_clouds))\n",
    "\n",
    "        xyzs = torch.zeros((n_mix, B, N, 3)).cuda()\n",
    "        for i in range(n_mix):\n",
    "            if i == 0: xyzs[i] = xyz\n",
    "            else:\n",
    "                xyzs[i] = xyz[idxs[i]]\n",
    "\n",
    "\n",
    "        all_xyz = torch.zeros((n_mix, B, N, 3)).cuda()\n",
    "        all_xyz[0] = xyzs[0]\n",
    "\n",
    "        all_saliency = torch.zeros((n_mix, B, N)).cuda()\n",
    "        all_saliency[0] = saliency\n",
    "        for i in range(1, n_mix):\n",
    "            _, ass = self.EMD(xyzs[0], xyzs[i], 0.005, 500)\n",
    "            #cast ass to long tensor\n",
    "            ass = ass.type(torch.long)\n",
    "\n",
    "            xyz_new = torch.zeros_like(xyzs[i]).cuda()\n",
    "            saliency_new = torch.zeros_like(saliency).cuda()\n",
    "            \n",
    "            \n",
    "            # print(ass,ass.shape)\n",
    "            for j in range(B):\n",
    "                # print(ass[j].dtype)\n",
    "                # print(\"ass j\", ass[j])\n",
    "                # print(\"ass j shape\", ass[j].shape)\n",
    "                # print(\"xyzs shape\", xyzs.shape)\n",
    "                # print(\"xyzs i shape\", xyzs[i].shape)\n",
    "                all_xyz[i][j] = xyzs[i][j][ass[j]]\n",
    "                all_saliency[i][j] = saliency[idxs[i]][j][ass[j]]\n",
    "\n",
    "                # xyz_new[i] = xyzs[j][ass[j]]\n",
    "                # saliency_new[j] = saliency[idxs[j]][j][ass[j]]\n",
    "            \n",
    "            # all_xyz[i] = xyz_new\n",
    "            # all_saliency[i] = saliency_new\n",
    "        # print(\"permuted saliency\", saliency[1])\n",
    "\n",
    "        anchors = torch.zeros(n_mix, B, 3).cuda()\n",
    "\n",
    "        saliency = saliency/saliency.sum(-1, keepdim=True)\n",
    "        anc_idx = torch.randint(0, 1024, (B,1)).cuda()\n",
    "        # anc_idx = torch.multinomial(saliency, 1, replacement=True)\n",
    "        anchor_ori = all_xyz[0][torch.arange(B), anc_idx[:,0]]\n",
    "        anchors[0] = anchor_ori\n",
    "        # # print(\"anchor shape\", anchor_ori.shape)\n",
    "\n",
    "        anc_idx_new = 0\n",
    "        perm_saliency_new = 0\n",
    "        # ker_weight_fix = 0\n",
    "        for i in range(1, n_mix):\n",
    "            dists = []\n",
    "            for j in range(0,i):\n",
    "                # print(\"all_xyz\", all_xyz[i])\n",
    "                # print(\"anchors\", anchors)\n",
    "                sub = all_xyz[i] - anchors[j][:, None, :]\n",
    "                # subs.append(sub)\n",
    "                dist = ((sub) ** 2).sum(2).sqrt()\n",
    "                dists.append(dist)\n",
    "                # print(dist.shape)\n",
    "            dist = torch.stack(dists).sum(dim=0)\n",
    "            \n",
    "            perm_saliency_new = all_saliency[i] * dist\n",
    "            perm_saliency_new = perm_saliency_new/perm_saliency_new.sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "        #     ## try to fix this at 0\n",
    "            # anc_idx_new = torch.multinomial(perm_saliency_new, 1, replacement=True)\n",
    "            anc_idx_new = torch.randint(0, 1024, (B,1)).cuda()\n",
    "            anchor_perm_new = all_xyz[i][torch.arange(B),anc_idx_new[:,0]]\n",
    "            anchors[i] = anchor_perm_new\n",
    "            # sub = perm_new - anchor_ori[:,None,:]\n",
    "        #     # dist = ((sub) ** 2).sum(2).sqrt()\n",
    "        #     # perm_saliency = perm_saliency * dist\n",
    "        #     # perm_saliency = perm_saliency/perm_saliency.sum(-1, keepdim=True)\n",
    "        # # alpha = self.dirichlet.sample((B,)).cuda()\n",
    "        pi = torch.distributions.dirichlet.Dirichlet(torch.tensor([theta for i in range(n_mix)])).sample((B,)).cuda()\n",
    "        # # print(\"pi shape\", pi.shape)\n",
    "        # # print(\"pi sum\", pi.sum(1))\n",
    "        \n",
    "\n",
    "        kerns = torch.zeros(n_mix, B, N).cuda()\n",
    "        weights = torch.zeros(n_mix, B, N).cuda()\n",
    "        weights_copy = []\n",
    "        for i in range(n_mix):\n",
    "            sub_ori = all_xyz[i] - anchors[i][:,None,:]\n",
    "            sub_ori = ((sub_ori) ** 2).sum(2).sqrt()\n",
    "        #     #Eq.(6) for first sample\n",
    "            ker_weight_ori = torch.exp(-0.5 * (sub_ori ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "            kerns[i] = ker_weight_ori\n",
    "        #     # print(\"kern weight ori\", ker_weight_ori.shape)\n",
    "\n",
    "            weights[i] = ker_weight_ori * pi[:,i][:,None]\n",
    "            weights_copy.append(weights[i][...,None])\n",
    "\n",
    "            # ker_weight_fix = ker_weight_ori\n",
    "\n",
    "\n",
    "        # # weight = (torch.cat([weight_ori[...,None],weight_perm[...,None]],-1)) + 1e-16\n",
    "        weight = (torch.cat(weights_copy,-1)) + 1e-16\n",
    "        weight = weight/weight.sum(-1)[...,None]\n",
    "\n",
    "        weight_old = weight.clone()\n",
    "        x = torch.zeros((B, N, 3)).cuda()\n",
    "\n",
    "        for i in range(n_mix):\n",
    "            x += weight[:, :, i:i+1] * all_xyz[i]\n",
    "        target = weight.sum(1)\n",
    "        target = target / target.sum(-1, keepdim=True)\n",
    "\n",
    "        label_one_hots = torch.zeros(n_mix, B, self.num_class).cuda()\n",
    "        label_onehot = torch.zeros(B, self.num_class).cuda().scatter(1, label.view(-1, 1), 1)\n",
    "        label_one_hots[0] = label_onehot\n",
    "        # print(\"label_onehot shape\", label_onehot.shape)\n",
    "\n",
    "        label = torch.zeros(B, self.num_class).cuda()\n",
    "        label += label_one_hots[0] * target[:, 0, None]\n",
    "        \n",
    "        for i in range(1, n_mix):\n",
    "            label_perm_onehot = label_onehot[idxs[i]]\n",
    "            label += label_perm_onehot * target[:, i, None]\n",
    "        \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(z, dist_type='l2'):\n",
    "    '''Return distance matrix between vectors'''\n",
    "    with torch.no_grad():\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        if dist_type[:2] == 'l2':\n",
    "            A_dist = (diff**2).sum(-1)\n",
    "            if dist_type == 'l2':\n",
    "                A_dist = torch.sqrt(A_dist)\n",
    "            elif dist_type == 'l22':\n",
    "                pass\n",
    "        elif dist_type == 'l1':\n",
    "            A_dist = diff.abs().sum(-1)\n",
    "        elif dist_type == 'linf':\n",
    "            A_dist = diff.abs().max(-1)[0]\n",
    "        else:\n",
    "            return None\n",
    "    return A_dist\n",
    "\n",
    "\n",
    "def calc_A_dist(saliency, theta=0.5):\n",
    "    sc = saliency.unsqueeze(1)\n",
    "    # print(\"sc:\",sc.shape)\n",
    "    # z = F.avg_pool1d(sc, kernel_size=8, stride=1)\n",
    "    # print(\"z:\",z.shape)\n",
    "    z = sc\n",
    "    z_reshape = z.reshape(args.batch_size, -1)\n",
    "    # print(\"z_reshape:\",z_reshape.shape)\n",
    "    z_idx_1d = torch.argmax(z_reshape, dim=1)\n",
    "    z_idx_2d = torch.zeros((args.batch_size, 2), device=z.device)\n",
    "    z_idx_2d[:, 0] = z_idx_1d // z.shape[-1]\n",
    "    z_idx_2d[:, 1] = z_idx_1d % z.shape[-1]\n",
    "    # print(\"z_idx_2d:\",z_idx_2d)\n",
    "    A_dist = distance(z_idx_2d, dist_type='l1')\n",
    "    # print(\"A_dist:\", A_dist)\n",
    "\n",
    "    n_input = saliency.shape[0]\n",
    "    \n",
    "    A_base = torch.eye(n_input, device=out.device)\n",
    "\n",
    "    A_dist = A_dist / torch.sum(A_dist) * n_input\n",
    "    m_omega = torch.distributions.beta.Beta(theta, theta).sample()\n",
    "    A = (1 - m_omega) * A_base + m_omega * A_dist\n",
    "    # print(\"A\", A)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_():\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    if not os.path.exists('checkpoints/'+args.exp_name):\n",
    "        os.makedirs('checkpoints/'+args.exp_name)\n",
    "    if not os.path.exists('checkpoints/'+args.exp_name+'/'+'models'):\n",
    "        os.makedirs('checkpoints/'+args.exp_name+'/'+'models')\n",
    "    os.system('cp main.py checkpoints'+'/'+args.exp_name+'/'+'main.py.backup')\n",
    "    os.system('cp model.py checkpoints' + '/' + args.exp_name + '/' + 'model.py.backup')\n",
    "    os.system('cp util.py checkpoints' + '/' + args.exp_name + '/' + 'util.py.backup')\n",
    "    os.system('cp data.py checkpoints' + '/' + args.exp_name + '/' + 'data.py.backup')\n",
    "\n",
    "def train(args, io):\n",
    "    if args.data == 'MN40':\n",
    "        dataset = ModelNet40(partition='train', num_points=args.num_points)\n",
    "        # args.batch_size = len(dataset)\n",
    "        args.batch_size = 24\n",
    "        print('args.batch_size:',args.batch_size)\n",
    "        train_loader = DataLoader(dataset, num_workers=8,\n",
    "                                batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(ModelNet40(partition='test', num_points=args.num_points), num_workers=8,\n",
    "                                batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "        num_class=40\n",
    "    elif args.data == 'SONN_easy':\n",
    "        train_loader = DataLoader(ScanObjectNN(partition='train', num_points=args.num_points, ver=\"easy\"), num_workers=8,\n",
    "                                batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(ScanObjectNN(partition='test', num_points=args.num_points, ver=\"easy\"), num_workers=8,\n",
    "                                batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "        num_class =15\n",
    "    elif args.data == 'SONN_hard':\n",
    "        train_loader = DataLoader(ScanObjectNN(partition='train', num_points=args.num_points, ver=\"hard\"), num_workers=8,\n",
    "                                batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(ScanObjectNN(partition='test', num_points=args.num_points, ver=\"hard\"), num_workers=8,\n",
    "                                batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "        num_class =15\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    #Try to load models\n",
    "    if args.model == 'pointnet':\n",
    "        model = PointNet(args, num_class).to(device)\n",
    "    elif args.model == 'dgcnn':\n",
    "        model = DGCNN(args, num_class).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "    print(str(model))\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if args.use_sgd:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "    \n",
    "\n",
    "    sagemix = SageMix(args, num_class)\n",
    "    criterion = cal_loss_mix\n",
    "\n",
    "    mixup = \"random\" if args.fixed_mixup is None else \"fixed {}\".format(args.fixed_mixup)\n",
    "\n",
    "    best_test_acc = 0\n",
    "    cnt = int(args.save_count)\n",
    "    print(\"count:\", cnt)\n",
    "    # return\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        for data, label in tqdm(train_loader):\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            # print(\"data shape\", data.shape)\n",
    "            batch_size = data.size()[0]\n",
    "            \n",
    "            ####################\n",
    "            # generate augmented sample\n",
    "            ####################\n",
    "            if args.fixed_mixup is None: \n",
    "                n_mix = random.randint(2,4)\n",
    "            else: \n",
    "                n_mix = int(args.fixed_mixup)\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(\"n_mix\", n_mix)\n",
    "            if n_mix > 1:\n",
    "                model.eval()\n",
    "                data_var = Variable(data.permute(0,2,1), requires_grad=True)\n",
    "                logits = model(data_var)\n",
    "                loss = cal_loss(logits, label, smoothing=False)\n",
    "                loss.backward()\n",
    "                opt.zero_grad()\n",
    "                saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "                \n",
    "                # saliency_based=False\n",
    "                # if args.mapping is not 'emd': saliency_based=True\n",
    "                if(epoch == 30):\n",
    "                    cnt -= 1\n",
    "                    if(cnt >= 0):\n",
    "                        \n",
    "                        print(\"count:\",cnt)\n",
    "                        print(\"saving data in epoch\", epoch,\"for batch count\", cnt)\n",
    "                        print(data.shape)\n",
    "                        # save data and label for visualization\n",
    "                        direc = \"Data_for_viz\"\n",
    "                        np.save(\"{}/data_unmixed_{}.npy\".format(direc,count), data.cpu().numpy())\n",
    "                        np.save(\"{}/label_unmixed_{}.npy\".format(direc,count), label.cpu().numpy())\n",
    "                data, label = sagemix.mix(data, label, saliency, n_mix)\n",
    "                if(epoch == 30 and cnt >= 0):\n",
    "                    np.save(\"{}/data_mixed_{}.npy\".format(direc,cnt), data.cpu().numpy())\n",
    "                    np.save(\"{}/label_mixed_{}.npy\".format(direc,cnt), label.cpu().numpy())\n",
    "                    #save the saliency matrix too\n",
    "                    np.save(\"{}/saliency_{}.npy\".format(direc,cnt), saliency.cpu().numpy())\n",
    "\n",
    "            \n",
    "                # mixed_saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "            # print(\"data shape\", data.shape)\n",
    "            # break\n",
    "            \n",
    "            model.train()\n",
    "            # break\n",
    "                \n",
    "            opt.zero_grad()\n",
    "            logits = model(data.permute(0,2,1))\n",
    "            if n_mix > 1:\n",
    "                loss = criterion(logits, label)\n",
    "            else:\n",
    "                loss = cal_loss(logits, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "        \n",
    "        scheduler.step()\n",
    "        outstr = 'Train %d, loss: %.6f' % (epoch, train_loss*1.0/count)\n",
    "        io.cprint(outstr)\n",
    "\n",
    "        ####################\n",
    "        # Test\n",
    "        ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in tqdm(test_loader):\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data = data.permute(0, 2, 1)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            loss = cal_loss(logits, label)\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f, best test acc: %.6f' % (epoch,\n",
    "                                                                              test_loss*1.0/count,\n",
    "                                                                              test_acc,\n",
    "                                                                              avg_per_class_acc,\n",
    "                                                                              best_test_acc)\n",
    "        \n",
    "        # wandb.log({\"Test acc\": test_acc, \"test avg acc\": avg_per_class_acc, \"best test acc\": best_test_acc, \"epoch\": epoch})\"})\n",
    "        # wandb.log({\"n_mix\": n_mix})\n",
    "        io.cprint(outstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='exp', fixed_mixup=None, k=20, lr=0.001, m_omega=0.9, mapping='emd', model='dgcnn', model_path='', momentum=0.9, no_cuda=False, num_points=1024, save_count='5', seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=True)\n",
      "Namespace(batch_size=32, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='exp', fixed_mixup=None, k=20, lr=0.001, m_omega=0.9, mapping='emd', model='dgcnn', model_path='', momentum=0.9, no_cuda=False, num_points=1024, save_count='5', seed=1, sigma=0.3, test_batch_size=16, theta=0.2, use_sgd=True)\n",
      "Using GPU : 0 from 6 devices\n",
      "args.batch_size: 24\n",
      "DGCNN(\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp2): Dropout(p=0.5, inplace=False)\n",
      "  (linear3): Linear(in_features=256, out_features=40, bias=True)\n",
      ")\n",
      "Let's use 6 GPUs!\n",
      "Use SGD\n",
      "count: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss: 3.492956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 42.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0, loss: 3.181736, test acc: 0.220827, test avg acc: 0.136250, best test acc: 0.220827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, loss: 3.104944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1, loss: 2.849949, test acc: 0.350486, test avg acc: 0.232227, best test acc: 0.350486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2, loss: 3.003427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:04<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2, loss: 2.818527, test acc: 0.364668, test avg acc: 0.256000, best test acc: 0.364668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 3, loss: 2.927179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3, loss: 2.664188, test acc: 0.426661, test avg acc: 0.306308, best test acc: 0.426661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 4, loss: 2.883504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 42.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4, loss: 2.625233, test acc: 0.487844, test avg acc: 0.348337, best test acc: 0.487844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5, loss: 2.825877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5, loss: 2.501624, test acc: 0.491896, test avg acc: 0.366285, best test acc: 0.491896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 6, loss: 2.792876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 44.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6, loss: 2.479428, test acc: 0.499595, test avg acc: 0.360058, best test acc: 0.499595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 7, loss: 2.740571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:04<00:00, 38.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7, loss: 2.369575, test acc: 0.546596, test avg acc: 0.397035, best test acc: 0.546596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 8, loss: 2.708007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 44.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8, loss: 2.276463, test acc: 0.648703, test avg acc: 0.521837, best test acc: 0.648703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 9, loss: 2.675695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:04<00:00, 38.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 9, loss: 2.403428, test acc: 0.572528, test avg acc: 0.460029, best test acc: 0.648703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10, loss: 2.625263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 43.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 10, loss: 2.283746, test acc: 0.683549, test avg acc: 0.574808, best test acc: 0.683549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 11, loss: 2.611473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 11, loss: 2.321327, test acc: 0.646272, test avg acc: 0.516256, best test acc: 0.683549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12, loss: 2.581792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 12, loss: 2.309464, test acc: 0.656807, test avg acc: 0.534169, best test acc: 0.683549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 13, loss: 2.572391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 39.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 13, loss: 2.318159, test acc: 0.658023, test avg acc: 0.551750, best test acc: 0.683549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14, loss: 2.542800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 43.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 14, loss: 2.254232, test acc: 0.694895, test avg acc: 0.572128, best test acc: 0.694895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15, loss: 2.551684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 40.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 15, loss: 2.298675, test acc: 0.674635, test avg acc: 0.565919, best test acc: 0.694895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 16, loss: 2.508439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 42.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 16, loss: 2.249105, test acc: 0.708671, test avg acc: 0.592640, best test acc: 0.708671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 17, loss: 2.492547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 17, loss: 2.350362, test acc: 0.670178, test avg acc: 0.547669, best test acc: 0.708671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 18, loss: 2.478147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 18, loss: 2.194626, test acc: 0.729741, test avg acc: 0.594192, best test acc: 0.729741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 19, loss: 2.454325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 43.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 19, loss: 2.246939, test acc: 0.714344, test avg acc: 0.607459, best test acc: 0.729741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 20, loss: 2.464678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 43.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 20, loss: 2.344118, test acc: 0.741086, test avg acc: 0.625965, best test acc: 0.741086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 21, loss: 2.417540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 38.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 21, loss: 2.331160, test acc: 0.730146, test avg acc: 0.616855, best test acc: 0.741086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 22, loss: 2.403279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 42.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 22, loss: 2.134927, test acc: 0.754862, test avg acc: 0.645884, best test acc: 0.754862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 23, loss: 2.403116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 23, loss: 2.380321, test acc: 0.758509, test avg acc: 0.668924, best test acc: 0.758509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 24, loss: 2.397855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 41.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 24, loss: 2.356671, test acc: 0.747974, test avg acc: 0.656035, best test acc: 0.758509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 25, loss: 2.346200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 42.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 25, loss: 2.257543, test acc: 0.778363, test avg acc: 0.669326, best test acc: 0.778363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 26, loss: 2.359623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 40.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 26, loss: 2.439531, test acc: 0.762561, test avg acc: 0.651070, best test acc: 0.778363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 27, loss: 2.349003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 39.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 27, loss: 2.218172, test acc: 0.779984, test avg acc: 0.674948, best test acc: 0.779984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:49<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 28, loss: 2.323850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 45.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 28, loss: 2.202474, test acc: 0.765397, test avg acc: 0.657628, best test acc: 0.779984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [01:48<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 29, loss: 2.310728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:03<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 29, loss: 2.130177, test acc: 0.781199, test avg acc: 0.681610, best test acc: 0.781199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 4\n",
      "saving data in epoch 30 for batch count 4\n",
      "torch.Size([24, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/410 [00:00<06:19,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 3\n",
      "saving data in epoch 30 for batch count 3\n",
      "torch.Size([24, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/410 [00:01<03:39,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 2\n",
      "saving data in epoch 30 for batch count 2\n",
      "torch.Size([24, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/410 [00:01<02:48,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 1\n",
      "saving data in epoch 30 for batch count 1\n",
      "torch.Size([24, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/410 [00:01<02:22,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 0\n",
      "saving data in epoch 30 for batch count 0\n",
      "torch.Size([24, 1024, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 231/410 [01:01<00:47,  3.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1158520/111515072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_mixup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# if not args.eval:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1158520/3241279480.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, io)\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/data_unmixed_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/label_unmixed_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaliency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/data_mixed_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1158520/3802892070.py\u001b[0m in \u001b[0;36mmix\u001b[0;34m(self, xyz, label, saliency, n_mix, theta)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mall_saliency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaliency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxyzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m#cast ass to long tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SageMix/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CV/project/MultiPointmixup/emd_/emd_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input1, input2, eps, iters)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0memdFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_emd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CV/project/MultiPointmixup/emd_/emd_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, xyz1, xyz2, eps, iters)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcnt_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0memd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxyz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massignment_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbid_increments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_increments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munass_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munass_cnt_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxyz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Point Cloud Recognition')\n",
    "parser.add_argument('--exp_name', type=str, default='exp', metavar='N',\n",
    "                    help='Name of the experiment')\n",
    "parser.add_argument('--model', type=str, default='dgcnn', metavar='N',\n",
    "                    choices=['pointnet', 'dgcnn'],\n",
    "                    help='Model to use, [pointnet, dgcnn]')\n",
    "parser.add_argument('--data', type=str, default='MN40', metavar='N',\n",
    "                    choices=['MN40', 'SONN_easy', 'SONN_hard']) #SONN_easy : OBJ_ONLY, SONN_hard : PB_T50_RS\n",
    "parser.add_argument('--batch_size', type=int, default=32, metavar='batch_size',\n",
    "                    help='Size of batch)')\n",
    "parser.add_argument('--test_batch_size', type=int, default=16, metavar='batch_size',\n",
    "                    help='Size of batch)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                    help='number of episode to train ')\n",
    "parser.add_argument('--use_sgd', type=bool, default=True,\n",
    "                    help='Use SGD')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001, 0.1 if using sgd)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--no_cuda', type=bool, default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--eval', type=bool,  default=False,\n",
    "                    help='evaluate the model')\n",
    "parser.add_argument('--num_points', type=int, default=1024,\n",
    "                    help='num of points to use')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='dropout rate')\n",
    "parser.add_argument('--emb_dims', type=int, default=1024, metavar='N',\n",
    "                    help='Dimension of embeddings')\n",
    "parser.add_argument('--k', type=int, default=20, metavar='N',\n",
    "                    help='Num of nearest neighbors to use')\n",
    "parser.add_argument('--m_omega', type=int, default=0.9,\n",
    "                    help='omega parameter')\n",
    "parser.add_argument('--mapping', type=str, default='emd',\n",
    "                    help='mapping function')\n",
    "parser.add_argument('--model_path', type=str, default='', metavar='N',\n",
    "                    help='Pretrained model path')\n",
    "parser.add_argument('--fixed_mixup', type=str, default=None, metavar='N',\n",
    "                    help='number of mixes')\n",
    "parser.add_argument('--save_count', type=str, default=\"5\", metavar='N')                    \n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--sigma', type=float, default=-1) \n",
    "parser.add_argument('--theta', type=float, default=0.2) \n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)\n",
    "if args.sigma==-1:\n",
    "    if args.model=='dgcnn':\n",
    "        args.sigma=0.3\n",
    "    elif args.model==\"pointnet\":\n",
    "        args.sigma=2.0\n",
    "\n",
    "_init_()\n",
    "\n",
    "if args.model=='dgcnn':\n",
    "    args.use_sgd=True\n",
    "elif args.model==\"pointnet\":\n",
    "    args.use_sgd=False\n",
    "\n",
    "io = IOStream('checkpoints/' + args.exp_name + '/run.log')\n",
    "io.cprint(str(args))\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    io.cprint(\n",
    "        'Using GPU : ' + str(torch.cuda.current_device()) + ' from ' + str(torch.cuda.device_count()) + ' devices')\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "else:\n",
    "    io.cprint('Using CPU')\n",
    "\n",
    "args.fixed_mixup = \"3\"\n",
    "train(args, io)\n",
    "\n",
    "# if not args.eval:\n",
    "#     train(args, io)\n",
    "# else:\n",
    "#     test(args, io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm Data_for_viz/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e43317eb3113a636e59ebf4e4d52ed79ac7360830f592e9d05ab9479dd90e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
