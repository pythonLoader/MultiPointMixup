{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from data_label_name import ModelNet40, ScanObjectNN\n",
    "from model import PointNet, DGCNN\n",
    "from util import cal_loss, cal_loss_mix, IOStream\n",
    "import gco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which GPUs to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "args = argparse.Namespace(batch_size=30, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.0001, model='pointnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1024\n",
    "dataset = ModelNet40(partition='train', num_points=num_points)\n",
    "batch_size=args.batch_size\n",
    "\n",
    "test_batch_size = args.test_batch_size\n",
    "train_loader = DataLoader(dataset, num_workers=8,\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', num_points=num_points), num_workers=8,\n",
    "                        batch_size=test_batch_size, shuffle=True, drop_last=False)\n",
    "num_class=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args = argparse.Namespace(batch_size=30, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.001, model='dgcnn', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=True)\n",
    "\n",
    "# args.cuda\n",
    "\n",
    "args.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'MN40':\n",
    "    dataset = ModelNet40(partition='train', num_points=args.num_points)\n",
    "    # args.batch_size = len(dataset)\n",
    "    # args.batch_size = 40\n",
    "    #print('args.batch_size:',args.batch_size)\n",
    "    train_loader = DataLoader(dataset, num_workers=8,\n",
    "                            batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(ModelNet40(partition='test', num_points=args.num_points), num_workers=8,\n",
    "                            batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "    num_class=40\n",
    "elif args.data == 'SONN_easy':\n",
    "    train_loader = DataLoader(ScanObjectNN(partition='train', num_points=args.num_points, ver=\"easy\"), num_workers=8,\n",
    "                            batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(ScanObjectNN(partition='test', num_points=args.num_points, ver=\"easy\"), num_workers=8,\n",
    "                            batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "    num_class =15\n",
    "elif args.data == 'SONN_hard':\n",
    "    train_loader = DataLoader(ScanObjectNN(partition='train', num_points=args.num_points, ver=\"hard\"), num_workers=8,\n",
    "                            batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(ScanObjectNN(partition='test', num_points=args.num_points, ver=\"hard\"), num_workers=8,\n",
    "                            batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
    "    num_class =15\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Try to load models\n",
    "if args.model == 'pointnet':\n",
    "    model = PointNet(args, num_class).to(device)\n",
    "elif args.model == 'dgcnn':\n",
    "    model = DGCNN(args, num_class).to(device)\n",
    "else:\n",
    "    raise Exception(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emd_ import emd_module\n",
    "import time\n",
    "torch.manual_seed(0)\n",
    "class SageMix:\n",
    "    def __init__(self, args, device, num_class=40):\n",
    "    \n",
    "        self.num_class = num_class\n",
    "        self.EMD = emd_module.emdModule()\n",
    "        self.sigma = args.sigma\n",
    "        # self.beta = torch.distributions.beta.Beta(torch.tensor([args.theta]), torch.tensor([args.theta]))\n",
    "        self.dirich = torch.distributions.dirichlet.Dirichlet(torch.tensor([args.theta,args.theta,args.theta]))\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def find_alignment_and_mapping(self,row):\n",
    "        MB = row.shape[0]\n",
    "        # print(\"MB:\",MB)\n",
    "        # dist_mat = torch.empty(MB, MB, 1024)\n",
    "        # ass_mat = torch.empty(MB,MB,1024)\n",
    "        # dist_mat = dist_mat.to(self.device)\n",
    "        \n",
    "        # print(\"Starting to compute optimal assignment (Heuristic-1)\")\n",
    "        \n",
    "        # perms = []\n",
    "        # rows = []\n",
    "        # for idx,point in enumerate(row):\n",
    "        #     # perm = torch.tensor([point for x in range(B))\n",
    "        #     # print(point.shape)\n",
    "        #     perm = point.repeat(MB,1)\n",
    "        #     # print('perm:',perm.shape)\n",
    "\n",
    "        #     perm  = perm.reshape(perm.shape[0]//1024,1024,3)\n",
    "        #     perms.append(perm)\n",
    "        #     rows.append(row)\n",
    "        #     start = time.time()\n",
    "        #     dist, ass = self.EMD(row, perm, 0.005, 500) # mapping\n",
    "        #          # 32,1024\n",
    "        #     # print('time---:', time.time() - start)\n",
    "        #     dist_mat[idx] = dist\n",
    "        #     ass_mat[idx] = ass\n",
    "\n",
    "\n",
    "       # Expand row into a (MB, MB, 1024, 3) tensor\n",
    "        row_expanded = row.unsqueeze(1).expand(-1, MB, -1, -1)\n",
    "        perm2 = row_expanded.clone()\n",
    "\n",
    "        row_repeated = torch.repeat_interleave(row.unsqueeze(0), repeats=MB, dim=0)\n",
    "        \n",
    "        # print(\"perm:\",torch.stack(perms))\n",
    "        # print(\"perm2:\",perm2)\n",
    "        # perms = torch.stack(perms)\n",
    "        # rows = torch.stack(rows)\n",
    "        # print(perms.shape,perm2.shape)\n",
    "        # print(\"row_expanded:\",row_expanded.shape)\n",
    "       \n",
    "        # print((perms == perm2).all())\n",
    "        # print((rows == row_repeated).all())\n",
    "        # start = time.time()\n",
    "        # print(\"rows_repeated reshape:\",row_repeated.reshape(-1,1024,3).shape)\n",
    "        # print(\"perm2 reshape:\",perm2.reshape(-1,1024,3).shape)\n",
    "        # print(\"row_expanded_shape:\",row_expanded.shape)\n",
    "        # print(\"perms\")\n",
    "        # Assuming self.EMD() can handle batches\n",
    "        dist2, ass2 = self.EMD(row_repeated.view(-1,1024,3),perm2.view(-1,1024,3), 0.005, 500) # mapping\n",
    "        # rete = [self.EMD(rows[i,...],perms[i,...], 0.005, 500) for i in range(MB)] # mapping\n",
    "\n",
    "        # dist3,ass3 = zip(*rete)\n",
    "        # dist3 = torch.stack(dist3)\n",
    "        # ass3 = torch.stack(ass3)\n",
    "        # print(\"dist3:\",dist3.shape)\n",
    "        # print(\"ass3:\",ass3.shape)\n",
    "\n",
    "        # print(\"dist2.shape:\",dist2.shape)\n",
    "        # print(\"ass2.shape:\",ass2.shape)\n",
    "        # print(\"dist.shape:\",dist_mat.shape)\n",
    "        # print(\"ass.shape\",ass.shape)\n",
    "        # print('time---:', time.time() - start)\n",
    "        # print(\"dist_mat:\",dist_mat)\n",
    "        \n",
    "        # Reshape dist and ass back to (MB, MB, 1024)\n",
    "        dist_mat = dist2.view(MB, MB, 1024)\n",
    "        ass_mat = ass2.view(MB, MB, 1024)\n",
    "\n",
    "        # dist_mat3 = dist3.view(MB,MB,1024)\n",
    "        # print((dist_mat2.cpu() == dist_mat3.cpu()).all())\n",
    "\n",
    "        # print(\"dist_mat2:\",dist_mat2)\n",
    "\n",
    "        # dist_mat2 = dist_mat2.permute((1,0,2))\n",
    "        # ass_mat2 = ass_mat2.permute((1,0,2))\n",
    "        # print(torch.where(torch.eq(dist_mat3.cpu(),dist_mat) == False))\n",
    "        # print(dist_mat3)\n",
    "\n",
    "        # print(\"dist_mat:\",dist_mat)\n",
    "        # print(\"dist_mat3:\",dist_mat3)\n",
    "\n",
    "\n",
    "        # print((ass_mat2.cpu() == ass_mat).all())\n",
    "        # print(\"dist_mat:\",dist_mat.shape,\"dist_mat2:\",dist_mat2.shape)\n",
    "    \n",
    "        # print(dist_mat.shape)\n",
    "        dist_mat = torch.norm(dist_mat,dim=2)\n",
    "        avg_alignment_dist = torch.mean(dist_mat,dim=0)\n",
    "        min_idx = torch.argmin(avg_alignment_dist).item()\n",
    "\n",
    "        return min_idx,ass_mat[min_idx]\n",
    "    \n",
    "        # print(\"dist matrix:\",dist_mat.shape)\n",
    "        # print(\"avg alignment dist:\",avg_alignment_dist)\n",
    "        # print(\"min index:\",min_idx)\n",
    "        # print(\"ass_mat at min idx:\",ass_mat[min_idx].shape)\n",
    "\n",
    "    def derangement(self,size):\n",
    "        while True:\n",
    "            perm = torch.randperm(size)\n",
    "            if (perm != torch.arange(size)).all():\n",
    "                return perm\n",
    "    \n",
    "\n",
    "    def mix(self, xyz, label, saliency=None, mixing_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xyz (B,N,3)\n",
    "            label (B)\n",
    "            saliency (B,N): Defaults to None.\n",
    "        \"\"\"        \n",
    "        # print(xyz.shape)\n",
    "        B, N, _ = xyz.shape\n",
    "        \n",
    "        # print(xyz.shape)\n",
    "        idxs1 = self.derangement(B)\n",
    "        idxs2 = self.derangement(B)\n",
    "        while True:\n",
    "            if (idxs1 != idxs2).all():\n",
    "                break\n",
    "            else:\n",
    "                idxs2 = self.derangement(B)\n",
    "        \n",
    "        perm1 = xyz[idxs1]\n",
    "        perm2 = xyz[idxs2]\n",
    "        optimal_pcd_idx = torch.empty(B)\n",
    "        # assignment = []\n",
    "        batch_pcd = torch.stack((xyz,perm1,perm2))\n",
    "    \n",
    "        # print(batch_pcd.shape)\n",
    "        batch_pcd = batch_pcd.permute(1,0,2,3)\n",
    "        # print(\"batch_pcd:\",batch_pcd.shape)\n",
    "        # self.find_alignment_and_mapping(row) for row in batch_pcd\n",
    "        # start = time.time()\n",
    "        ret = [self.find_alignment_and_mapping(row) for row in batch_pcd] # [3,1024,3]\n",
    "        \n",
    "        # print('time for finding optimal alignment: ', time.time() - start)\n",
    "        # return \n",
    "        # print(ret)\n",
    "        ret_1, ret_2 = zip(*ret)\n",
    "        optimal_pcd_idx = torch.tensor(ret_1)\n",
    "\n",
    "        assignment = torch.stack(ret_2)\n",
    "\n",
    "        # print(\"index array:\",optimal_pcd_idx.shape)\n",
    "        # print(\"assignment:\",assignment.shape)\n",
    "        # return\n",
    "        #Optimal assignment in Eq.(3)\n",
    "        \n",
    "        assignment = assignment.long()\n",
    "        xyz_new = torch.zeros_like(xyz).cuda()\n",
    "        xyz_saliency = torch.zeros_like(saliency).cuda()\n",
    "\n",
    "        perm1_new = torch.zeros_like(perm1).cuda()\n",
    "        perm1_saliency = torch.zeros_like(saliency).cuda()\n",
    "\n",
    "        perm2_new = torch.zeros_like(perm2).cuda()\n",
    "        perm2_saliency = torch.zeros_like(saliency).cuda()\n",
    "\n",
    "        # print(assignment[:,0,:].dtype,assignment[:,0,:].shape)\n",
    "        # print(\"saliency mat:\",saliency.shape)\n",
    "\n",
    "        for i in range(B):\n",
    "            xyz_new[i] = xyz[i][assignment[i,0]]\n",
    "            # print(saliency[i].shape,assignment[i,0].shape)\n",
    "            xyz_saliency[i] = saliency[i][assignment[i,0]]\n",
    "            perm1_new[i] = perm1[i][assignment[i,1]]\n",
    "            perm1_saliency[i] = saliency[i][assignment[i,1]]\n",
    "            perm2_new[i] = perm2[i][assignment[i,2]]\n",
    "            perm2_saliency[i] = saliency[i][assignment[i,2]]\n",
    "        \n",
    "\n",
    "        # xyz_new = xyz[assignment[:,0,:]]\n",
    "        # perm1_new = perm1[assignment[:,1,:]]\n",
    "        # perm2_new = perm2[assignment[:,2,:]]\n",
    "        \n",
    "        # print(ass,ass.shape)\n",
    "        # for i in range(B):\n",
    "        #     perm1_new[i] = perm[i][ass[i]]\n",
    "        #     perm_saliency[i] = saliency[idxs][i][ass[i]]\n",
    "        # print(\"Three rotations:\",xyz_new.shape,perm1_new.shape,perm2_new.shape)\n",
    "        # return\n",
    "        #####\n",
    "        # Saliency-guided sequential sampling\n",
    "        #####\n",
    "        #Eq.(4) in the main paper\n",
    "        xyz_saliency = xyz_saliency/xyz_saliency.sum(-1, keepdim=True)\n",
    "        anc_idx = torch.multinomial(xyz_saliency, 1, replacement=True)\n",
    "        anchor_ori = xyz_new[torch.arange(B), anc_idx[:,0]]\n",
    "\n",
    "\n",
    "        # print(\"anchor ori:\",anchor_ori.shape)\n",
    "        #cal distance and reweighting saliency map for Eq.(5) in the main paper\n",
    "        sub = perm1_new - anchor_ori[:,None,:]\n",
    "        dist = ((sub) ** 2).sum(2).sqrt()\n",
    "        perm1_saliency = perm1_saliency * dist\n",
    "        perm1_saliency = perm1_saliency/perm1_saliency.sum(-1, keepdim=True)\n",
    "        # print(\"perm1_saliency:\",perm1_saliency)\n",
    "        \n",
    "        #Eq.(5) in the main paper\n",
    "        anc_idx2 = torch.multinomial(perm1_saliency, 1, replacement=True)\n",
    "        anchor_perm1 = perm1_new[torch.arange(B),anc_idx2[:,0]]\n",
    "\n",
    "        # print(\"anchor_perm:\",anchor_perm1.shape)\n",
    "        \n",
    "        sub21 = (perm2_new - anchor_ori[:,None,:])\n",
    "        dist21 = ((sub21) ** 2).sum(2).sqrt()\n",
    "        sub22 = (perm2_new - anchor_perm1[:,None,:])\n",
    "        dist22 = ((sub22) ** 2).sum(2).sqrt()\n",
    "        perm2_saliency = perm2_saliency * dist21 + perm2_saliency * dist22\n",
    "        perm2_saliency = perm2_saliency/perm2_saliency.sum(-1, keepdim=True)\n",
    "        # print(\"perm2_saliency:\",perm2_saliency)\n",
    "\n",
    "        #Eq.(5) in the main paper\n",
    "        anc_idx3 = torch.multinomial(perm2_saliency, 1, replacement=True)\n",
    "        anchor_perm2 = perm2_new[torch.arange(B),anc_idx3[:,0]]\n",
    "\n",
    "    \n",
    "        # return\n",
    "        #####\n",
    "        # Shape-preserving continuous Mixup\n",
    "        #####\n",
    "        pi = self.dirich.sample((B,)).cuda()\n",
    "        # print(\"pi sum\",pi.sum(dim=1))\n",
    "\n",
    "\n",
    "        # return\n",
    "        sub_ori = xyz_new - anchor_ori[:,None,:]\n",
    "        sub_ori = ((sub_ori) ** 2).sum(2).sqrt()\n",
    "        #Eq.(6) for first sample\n",
    "        ker_weight_ori = torch.exp(-0.5 * (sub_ori ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "        \n",
    "        sub_perm1 = perm1_new - anchor_perm1[:,None,:]\n",
    "        sub_perm1 = ((sub_perm1) ** 2).sum(2).sqrt()\n",
    "        #Eq.(6) for second sample\n",
    "        ker_weight_perm1 = torch.exp(-0.5 * (sub_perm1 ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "\n",
    "        sub_perm2 = perm2_new - anchor_perm2[:,None,:]\n",
    "        sub_perm2 = ((sub_perm2) ** 2).sum(2).sqrt()\n",
    "        #Eq.(6) for third sample\n",
    "        ker_weight_perm2 = torch.exp(-0.5 * (sub_perm2 ** 2) / (self.sigma ** 2))  #(M,N)\n",
    "\n",
    "        \n",
    "        # print(\"ker_weight_ori:\",ker_weight_ori.shape)\n",
    "        #Eq.(9)\n",
    "        weight_ori = ker_weight_ori * pi[:,0][:,None]\n",
    "        weight_perm1 = ker_weight_perm1 * pi[:,1][:,None]\n",
    "        weight_perm2 = ker_weight_perm2 * pi[:,2][:,None]\n",
    "\n",
    "        weight = (torch.cat([weight_ori[...,None],weight_perm1[...,None],weight_perm2[...,None]],-1)) + 1e-16\n",
    "        weight = weight/weight.sum(-1)[...,None]\n",
    "        # print(\"weight:\",weight.shape)\n",
    "        #Eq.(8) for new sample\n",
    "        x = weight[:,:,0:1] * xyz_new + weight[:,:,1:2] * perm1_new + weight[:,:,2:] * perm2_new\n",
    "        \n",
    "        #Eq.(8) for new sample\n",
    "        # x = weight[:,:,0:1] * xyz + weight[:,:,1:] * perm_new\n",
    "        \n",
    "        #Eq.(8) for new label\n",
    "        target = weight.sum(1)\n",
    "        target = target / target.sum(-1, keepdim=True)\n",
    "        # print(\"label shape\",label.shape)\n",
    "        # print(self.num_class)\n",
    "        label_onehot = torch.zeros(B, self.num_class).cuda().scatter(1, label.view(-1, 1), 1)\n",
    "        label_perm1_onehot = label_onehot[idxs1]\n",
    "        label_perm2_onehot = label_onehot[idxs2]\n",
    "        label = target[:, 0, None] * label_onehot + target[:, 1, None] * label_perm1_onehot + target[:, 2, None] * label_perm2_onehot\n",
    "\n",
    "        # print(\"label new shape:\",label.shape)\n",
    "\n",
    "        \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Check if multiple GPUs are available and wrap the model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=30, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.0001, model='pointnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/328 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "io = IOStream('checkpoints/' + args.exp_name + '/run.log')\n",
    "io.cprint(str(args))\n",
    "if args.use_sgd:\n",
    "    #print(\"Use SGD\")\n",
    "    opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "else:\n",
    "    #print(\"Use Adam\")\n",
    "    opt = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "sagemix=SageMix(args,device,num_class=num_class)\n",
    "scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "criterion = cal_loss_mix\n",
    "\n",
    "best_test_acc = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    ####################\n",
    "    # Train\n",
    "    ####################\n",
    "    train_loss = 0.0\n",
    "    count = 0.0\n",
    "    model.train()\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    for data, label in tqdm(train_loader):\n",
    "        # start = time.time()\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        # print(\"data shape\", data.shape)\n",
    "        batch_size = data.size()[0]\n",
    "        \n",
    "        ####################\n",
    "        # generate augmented sample\n",
    "        ####################\n",
    "        model.eval()\n",
    "        data_var = Variable(data.permute(0,2,1), requires_grad=True)\n",
    "        logits = model(data_var)\n",
    "        loss = cal_loss(logits, label, smoothing=False)\n",
    "        loss.backward()\n",
    "        opt.zero_grad()\n",
    "        saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "        data, label = sagemix.mix(data, label, saliency)\n",
    "\n",
    "        # break\n",
    "        \n",
    "        mixed_saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "        # print(\"data shape\", data.shape)\n",
    "        model.train()\n",
    "        # break\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        logits = model(data.permute(0,2,1))\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        count += batch_size\n",
    "        train_loss += loss.item() * batch_size\n",
    "        # print('time of batch:', time.time() - start)\n",
    "    # break \n",
    "    scheduler.step()\n",
    "    outstr = 'Train %d, loss: %.6f' % (epoch, train_loss*1.0/count)\n",
    "    io.cprint(outstr)\n",
    "\n",
    "    ####################\n",
    "    # Test\n",
    "    ####################\n",
    "    test_loss = 0.0\n",
    "    count = 0.0\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_true = []\n",
    "    for data, label in tqdm(test_loader):\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        loss = cal_loss(logits, label)\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        count += batch_size\n",
    "        test_loss += loss.item() * batch_size\n",
    "        test_true.append(label.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    if test_acc >= best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
    "    outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f, best test acc: %.6f' % (epoch,\n",
    "                                                                            test_loss*1.0/count,\n",
    "                                                                            test_acc,\n",
    "                                                                            avg_per_class_acc,\n",
    "                                                                            best_test_acc)\n",
    "    io.cprint(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 | packaged by conda-forge | (default, Mar  5 2020, 15:27:18) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e43317eb3113a636e59ebf4e4d52ed79ac7360830f592e9d05ab9479dd90e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
