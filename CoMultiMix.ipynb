{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssawmya-local/.conda/envs/SageMix/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from SageMix import SageMix\n",
    "from data import ModelNet40, ScanObjectNN\n",
    "from model import PointNet, DGCNN\n",
    "from util import cal_loss, cal_loss_mix, IOStream\n",
    "import gco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(batch_size=5, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.0001, model='pointnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1024\n",
    "dataset = ModelNet40(partition='train', num_points=num_points)\n",
    "batch_size=args.batch_size\n",
    "# print(dataset)\n",
    "# dataset = dataset[:100]\n",
    "# batch_size = len(dataset)\n",
    "# print('args.batch_size:',batch_size)\n",
    "test_batch_size = args.test_batch_size\n",
    "train_loader = DataLoader(dataset, num_workers=8,\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', num_points=num_points), num_workers=8,\n",
    "                        batch_size=test_batch_size, shuffle=True, drop_last=False)\n",
    "num_class=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emd_ import emd_module\n",
    "\n",
    "class SageMix:\n",
    "    def __init__(self, args, device, num_class=40):\n",
    "        self.num_class = num_class\n",
    "        self.EMD = emd_module.emdModule()\n",
    "        self.sigma = args.sigma\n",
    "        self.beta = torch.distributions.beta.Beta(torch.tensor([args.theta]), torch.tensor([args.theta]))\n",
    "        self.device = device\n",
    "\n",
    "    def mix(self, xyz, label, saliency=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xyz (B,N,3)\n",
    "            label (B)\n",
    "            saliency (B,N): Defaults to None.\n",
    "        \"\"\"        \n",
    "        B, N, _ = xyz.shape\n",
    "        # print(xyz.shape)\n",
    "        idxs = torch.randperm(B)\n",
    "\n",
    "        \n",
    "        #Optimal assignment in Eq.(3)\n",
    "        # perm = xyz[idxs]\n",
    "        dist_mat = torch.empty(B, B, 1024)\n",
    "        ass_mat = torch.empty(B,B,1024)\n",
    "        dist_mat = dist_mat.to(self.device)\n",
    "        \n",
    "        # print(\"Starting to compute optimal assignment (Heuristic-1)\")\n",
    "        for idx,point in enumerate(xyz):\n",
    "            # perm = torch.tensor([point for x in range(B))\n",
    "            # print(point.shape)\n",
    "            perm = point.repeat(B,1)\n",
    "            # print(perm.shape)\n",
    "\n",
    "            perm  = perm.reshape(perm.shape[0]//1024,1024,3)\n",
    "            \n",
    "            dist, ass = self.EMD(xyz, perm, 0.005, 500) # mapping\n",
    "                 # 32,1024\n",
    "            dist_mat[idx] = dist\n",
    "            ass_mat[idx] = ass\n",
    "\n",
    "            # print('dist:',dist.shape)\n",
    "            # if idx % 10 == 0:\n",
    "            #     print(\"Now doing\", idx)\n",
    "        \n",
    "        # print(dist_mat.shape)\n",
    "        dist_mat = torch.norm(dist_mat,dim=2)\n",
    "        avg_alignment_dist = torch.mean(dist_mat,dim=0)\n",
    "        # print(avg_alignment_dist.shape)\n",
    "        # print('avg_alignment:',avg_alignment_dist)\n",
    "        # print('mean:',torch.mean(avg_alignment_dist))\n",
    "        # print('min:',torch.min(avg_alignment_dist))\n",
    "        # print('max:',torch.max(avg_alignment_dist))\n",
    "        # print(torch.min(avg_alignment_dist))\n",
    "        # print(torch.argmin(avg_alignment_dist).item())\n",
    "\n",
    "        idx = torch.argmin(avg_alignment_dist).item()\n",
    "        # dist_mat = dist_mat.fill_diagonal_(100000.0)\n",
    "    \n",
    "        \n",
    "        # i,j = divmod(torch.argmin(dist_mat).item(),dist_mat.shape[1])\n",
    "        ass = ass_mat[idx]\n",
    "        \n",
    "        ass = ass.long()\n",
    "\n",
    "        # sz = ass.size(0)\n",
    "        perm_new = torch.zeros_like(perm).to(self.device)\n",
    "        # print('perm:',perm)\n",
    "        # print(perm_new.shape)\n",
    "        perm = xyz.clone()\n",
    "        # print(\"idx:\",idx)\n",
    "        for i in range(B):\n",
    "            # print('i:',i)\n",
    "            perm_new[i] = perm[i][ass[i]]\n",
    "            # print('perm_i',perm[i])\n",
    "            # print('perm_new_i',perm_new[i])\n",
    "\n",
    "        # print('perm_new',perm_new)\n",
    "\n",
    "        return ass,perm_new,dist_mat\n",
    "\n",
    "        # print(\"Done with compute optimal assignment (Heuristic-1)\")\n",
    "        # print(ass.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(z, dist_type='l2'):\n",
    "    '''Return distance matrix between vectors'''\n",
    "    with torch.no_grad():\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        if dist_type[:2] == 'l2':\n",
    "            A_dist = (diff**2).sum(-1)\n",
    "            if dist_type == 'l2':\n",
    "                A_dist = torch.sqrt(A_dist)\n",
    "            elif dist_type == 'l22':\n",
    "                pass\n",
    "        elif dist_type == 'l1':\n",
    "            A_dist = diff.abs().sum(-1)\n",
    "        elif dist_type == 'linf':\n",
    "            A_dist = diff.abs().max(-1)[0]\n",
    "        else:\n",
    "            return None\n",
    "    return A_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import importlib\n",
    "import match\n",
    "from math import ceil\n",
    "\n",
    "importlib.reload(match)\n",
    "from match import get_onehot_matrix, mix_input\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def mixup_process(out, target_reweighted, args=None, sc=None, A_dist=None):\n",
    "    m_block_num = args.m_block_num\n",
    "    m_part = args.m_part\n",
    "\n",
    "    # batch_size = out.shape[0]\n",
    "    # width = out.shape[-1]\n",
    "\n",
    "    if A_dist is None:\n",
    "        A_dist = torch.eye(batch_size, device=out.device)\n",
    "\n",
    "    if m_block_num == -1:\n",
    "        m_block_num = 2**np.random.randint(1, 5)\n",
    "\n",
    "    \n",
    "    # block_size = width // m_block_num\n",
    "    block_size = 8\n",
    "    print(\"block size:\",block_size)\n",
    "    print(\"sc:\",sc.shape) # 8,1024\n",
    "    # sc = sc.unsqueeze(1)\n",
    "    # sc = F.avg_pool1d(sc, block_size)\n",
    "\n",
    "\n",
    "    out_list = []\n",
    "    target_list = []\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print()\n",
    "        sc_part = sc\n",
    "        A_dist_part = A_dist\n",
    "\n",
    "        n_input = sc.shape[0]\n",
    "        sc_norm = sc/torch.sum(sc, dim=1).view(-1,1)\n",
    "\n",
    "        # sc_norm = sc_part / sc_part.reshape(n_input, -1).sum(1).reshape(n_input, 1, 1)\n",
    "        print(\"sc norm:\",sc_norm.shape)\n",
    "        print(\"sc norm sum\",torch.sum(sc_norm,dim=1))\n",
    "        cost_matrix = -sc_norm\n",
    "\n",
    "        A_base = torch.eye(n_input, device=out.device)\n",
    "        A_dist_part = A_dist_part / torch.sum(A_dist_part) * n_input\n",
    "        A = (1 - args.m_omega) * A_base + args.m_omega * A_dist_part\n",
    "        print(\"cost matrix shape:\",cost_matrix.shape)\n",
    "\n",
    "        print(\"new A shape:\",A.shape)\n",
    "        # Return a batch(partitioned) of mixup labeling\n",
    "        mask_onehot = get_onehot_matrix(cost_matrix.detach(),\n",
    "                                        A,\n",
    "                                        n_output=n_input,\n",
    "                                        beta=args.m_beta,\n",
    "                                        gamma=args.m_gamma,\n",
    "                                        eta=args.m_eta,\n",
    "                                        mixup_alpha=args.mixup_alpha,\n",
    "                                        thres=args.m_thres,\n",
    "                                        thres_type=args.m_thres_type,\n",
    "                                        set_resolve=args.set_resolve,\n",
    "                                        niter=args.m_niter,\n",
    "                                        device='cuda')\n",
    "    print('mask onehot shape:',mask_onehot.shape)\n",
    "    # print(mask_onehot)\n",
    "    # Generate image and corrsponding soft target\n",
    "    output_part, target_part = mix_input(mask_onehot, out,\n",
    "                                             target_reweighted)\n",
    "\n",
    "    out_list = output_part\n",
    "    target_list = target_part\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = torch.cat(out_list, dim=0)\n",
    "        target_reweighted = torch.cat(target_list, dim=0)\n",
    "\n",
    "    return out.contiguous(), target_reweighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args2 = {'arch': 'preactresnet18', 'batch_size': 100, \n",
    "         'clean_lam': 1.0, 'comix': True, \n",
    "         'data_dir': './data/cifar100/', 'dataset': 'cifar100', \n",
    "         'decay': 0.0001, 'dropout': False, 'epochs': 300, \n",
    "         'evaluate': True, 'gammas': [0.1, 0.1], 'initial_channels': 64, \n",
    "         'labels_per_class': 500, 'learning_rate': 0.2, \n",
    "         'log_off': True, 'm_beta': 0.32, \n",
    "         'm_block_num': 4, 'm_eta': 0.05, \n",
    "         'm_gamma': 1.0, 'm_niter': 4, 'm_omega': 0.001, \n",
    "         'm_part': 20, 'm_thres': 0.83, \n",
    "         'm_thres_type': 'hard', \n",
    "         'mixup_alpha': 2.0, \n",
    "         'momentum': 0.9, 'ngpu': 1, \n",
    "         'parallel': False, 'print_freq': 100, \n",
    "         'resume': './checkpoint/cifar100_preactresnet18_eph300_comixup/checkpoint.pth.tar', \n",
    "         'root_dir': 'experiments', 'schedule': [100, 200], 'seed': 0, \n",
    "         'set_resolve': True, 'start_epoch': 0, 'tag': '', \n",
    "         'use_cuda': True, 'valid_labels_per_class': 0, 'workers': 0}\n",
    "\n",
    "args2 = argparse.Namespace(**args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(args2.m_block_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 6 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1968 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1968 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1024, 3])\n",
      "torch.Size([5, 1024])\n",
      "sc: torch.Size([5, 1, 1024])\n",
      "z: torch.Size([5, 1, 1017])\n",
      "z_reshape: torch.Size([5, 1017])\n",
      "A_dist: torch.Size([5, 5])\n",
      "tensor([[  0., 997., 138., 339., 783.],\n",
      "        [997.,   0., 859., 658., 214.],\n",
      "        [138., 859.,   0., 201., 645.],\n",
      "        [339., 658., 201.,   0., 444.],\n",
      "        [783., 214., 645., 444.,   0.]], device='cuda:0')\n",
      "block size: 8\n",
      "sc: torch.Size([5, 1024])\n",
      "\n",
      "sc norm: torch.Size([5, 1024])\n",
      "sc norm sum tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "cost matrix shape: torch.Size([5, 1024])\n",
      "new A shape: torch.Size([5, 5])\n",
      "cost matrix shape inside get onehot matrix: torch.Size([5, 1024])\n",
      "alpha shape: torch.Size([5, 1])\n",
      "eta shape 4.8828125e-05\n",
      "cost shape torch.Size([5, 1024])\n",
      "mask shape torch.Size([5, 1024, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [5, 1, 1] doesn't match the broadcast shape [5, 5, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3243526/2279221460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                                                 \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaliency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                                 A_dist=A_dist)\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# model.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3243526/371575549.py\u001b[0m in \u001b[0;36mmixup_process\u001b[0;34m(out, target_reweighted, args, sc, A_dist)\u001b[0m\n\u001b[1;32m     70\u001b[0m                                         \u001b[0mset_resolve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_resolve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         \u001b[0mniter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_niter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                                         device='cuda')\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask onehot shape:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# print(mask_onehot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CV/project/sagemix_new/SageMix/pointcloud/match.py\u001b[0m in \u001b[0;36mget_onehot_matrix\u001b[0;34m(cost_matrix, A, n_output, idx, beta, gamma, eta, mixup_alpha, thres, thres_type, set_resolve, niter, device)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mlabel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mpenalty\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlabel_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mthres_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'hard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     modular_penalty = (2 * gamma * (\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [5, 1, 1] doesn't match the broadcast shape [5, 5, 1]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = PointNet(args, num_class).to(device)\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "    \n",
    "\n",
    "sagemix = SageMix(args, device, num_class)\n",
    "criterion = cal_loss_mix\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "args.epochs = 1\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    ####################\n",
    "    # Train\n",
    "    ####################\n",
    "    train_loss = 0.0\n",
    "    count = 0.0\n",
    "    model.train()\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        batch_size = data.size()[0]\n",
    "        \n",
    "        ####################\n",
    "        # generate augmented sample\n",
    "        ####################\n",
    "        model.eval()\n",
    "        print(data.permute(0,2,1).shape)\n",
    "        data_var = Variable(data.permute(0,2,1), requires_grad=True)\n",
    "        logits = model(data_var)\n",
    "        loss = cal_loss(logits, label, smoothing=False)\n",
    "        loss.backward()\n",
    "        opt.zero_grad()\n",
    "        saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "        \n",
    "        assignment,perm_new,align_dist = sagemix.mix(data, label, saliency)\n",
    "        print(perm_new.shape)\n",
    "        # data_var2 = Variable(perm_new.permute(0,2,1), requires_grad=True)\n",
    "        \n",
    "        # sc = torch.sqrt(torch.mean(data_var2.grad**2,1))\n",
    "        # print('assignment:',assignment.shape)\n",
    "        # print(assignment[0])\n",
    "        # print('new_permutation',perm_new.shape)\n",
    "        \n",
    "        target_reweighted = F.one_hot(label, num_classes=num_class).float()\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(saliency.shape)\n",
    "            sc = saliency.unsqueeze(1)\n",
    "            print(\"sc:\",sc.shape)\n",
    "            z = F.avg_pool1d(sc, kernel_size=8, stride=1)\n",
    "            print(\"z:\",z.shape)\n",
    "            z_reshape = z.reshape(args.batch_size, -1)\n",
    "            print(\"z_reshape:\",z_reshape.shape)\n",
    "            z_idx_1d = torch.argmax(z_reshape, dim=1)\n",
    "            z_idx_2d = torch.zeros((args.batch_size, 2), device=z.device)\n",
    "            z_idx_2d[:, 0] = z_idx_1d // z.shape[-1]\n",
    "            z_idx_2d[:, 1] = z_idx_1d % z.shape[-1]\n",
    "            A_dist = distance(z_idx_2d, dist_type='l1')\n",
    "            print(\"A_dist:\",A_dist.shape)\n",
    "            print(A_dist)\n",
    "\n",
    "        # print(A_dist)\n",
    "        out, target_reweighted = mixup_process(perm_new,\n",
    "                                                target_reweighted,\n",
    "                                                args=args2,\n",
    "                                                sc=saliency,\n",
    "                                                A_dist=A_dist)\n",
    "        break\n",
    "        # model.train()\n",
    "            \n",
    "        # opt.zero_grad()\n",
    "        # logits = model(data.permute(0,2,1))\n",
    "        # loss = criterion(logits, label)\n",
    "        # loss.backward()\n",
    "        # opt.step()\n",
    "        # preds = logits.max(dim=1)[1]\n",
    "        # count += batch_size\n",
    "        # train_loss += loss.item() * batch_size\n",
    "        \n",
    "    scheduler.step()\n",
    "    outstr = 'Train %d, loss: %.6f' % (epoch, train_loss*1.0/count)\n",
    "    print(outstr)\n",
    "    # io.cprint(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e43317eb3113a636e59ebf4e4d52ed79ac7360830f592e9d05ab9479dd90e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
