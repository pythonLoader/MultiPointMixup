{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from SageMix import SageMix\n",
    "from data import ModelNet40, ScanObjectNN\n",
    "from model import PointNet, DGCNN\n",
    "from util import cal_loss, cal_loss_mix, IOStream\n",
    "import gco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr(300) = 0 + 0.5 * (0.01 - 0) * (1 + cos(π * 300 / 400))\n",
    "# scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "\n",
    "lr = 0 + 0.5 * (0.1 - 0.001) * (1 + np.cos(np.pi * 365 / 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016765062666479223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(batch_size=32, data='MN40', dropout=0.5, emb_dims=1024, epochs=50, eval=False, exp_name='SageMix', k=20, lr=0.0001, model='pointnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, sigma=-1, test_batch_size=16, theta=0.2, use_sgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1024\n",
    "dataset = ModelNet40(partition='train', num_points=num_points)\n",
    "batch_size=args.batch_size\n",
    "# print(dataset)\n",
    "# dataset = dataset[:100]\n",
    "# batch_size = len(dataset)\n",
    "# print('args.batch_size:',batch_size)\n",
    "test_batch_size = args.test_batch_size\n",
    "train_loader = DataLoader(dataset, num_workers=8,\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', num_points=num_points), num_workers=8,\n",
    "                        batch_size=test_batch_size, shuffle=True, drop_last=False)\n",
    "num_class=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from emd_ import emd_module\n",
    "\n",
    "class SageMix:\n",
    "    def __init__(self, args, device, num_class=40):\n",
    "        self.num_class = num_class\n",
    "        self.EMD = emd_module.emdModule()\n",
    "        self.sigma = args.sigma\n",
    "        self.beta = torch.distributions.beta.Beta(torch.tensor([args.theta]), torch.tensor([args.theta]))\n",
    "        self.device = device\n",
    "\n",
    "    def mix(self, xyz, label, saliency=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xyz (B,N,3)\n",
    "            label (B)\n",
    "            saliency (B,N): Defaults to None.\n",
    "        \"\"\"        \n",
    "        B, N, _ = xyz.shape\n",
    "        # print(xyz.shape)\n",
    "        idxs = torch.randperm(B)\n",
    "\n",
    "        \n",
    "        #Optimal assignment in Eq.(3)\n",
    "        # perm = xyz[idxs]\n",
    "        dist_mat = torch.empty(B, B, 1024)\n",
    "        ass_mat = torch.empty(B,B,1024)\n",
    "        dist_mat = dist_mat.to(self.device)\n",
    "        \n",
    "        # print(\"Starting to compute optimal assignment (Heuristic-1)\")\n",
    "        for idx,point in enumerate(xyz):\n",
    "            # perm = torch.tensor([point for x in range(B))\n",
    "            # print(point.shape)\n",
    "            perm = point.repeat(B,1)\n",
    "            # print(perm.shape)\n",
    "\n",
    "            perm  = perm.reshape(perm.shape[0]//1024,1024,3)\n",
    "            \n",
    "            dist, ass = self.EMD(xyz, perm, 0.005, 500) # mapping\n",
    "                 # 32,1024\n",
    "            dist_mat[idx] = dist\n",
    "            ass_mat[idx] = ass\n",
    "\n",
    "            # print('dist:',dist.shape)\n",
    "            # if idx % 10 == 0:\n",
    "            #     print(\"Now doing\", idx)\n",
    "        \n",
    "        # print(dist_mat.shape)\n",
    "        dist_mat = torch.norm(dist_mat,dim=2)\n",
    "        avg_alignment_dist = torch.mean(dist_mat,dim=0)\n",
    "        # print(avg_alignment_dist.shape)\n",
    "        # print('avg_alignment:',avg_alignment_dist)\n",
    "        # print('mean:',torch.mean(avg_alignment_dist))\n",
    "        # print('min:',torch.min(avg_alignment_dist))\n",
    "        # print('max:',torch.max(avg_alignment_dist))\n",
    "        # print(torch.min(avg_alignment_dist))\n",
    "        # print(torch.argmin(avg_alignment_dist).item())\n",
    "\n",
    "        idx = torch.argmin(avg_alignment_dist).item()\n",
    "        # dist_mat = dist_mat.fill_diagonal_(100000.0)\n",
    "    \n",
    "        \n",
    "        # i,j = divmod(torch.argmin(dist_mat).item(),dist_mat.shape[1])\n",
    "        ass = ass_mat[idx]\n",
    "        \n",
    "        ass = ass.long()\n",
    "\n",
    "        # sz = ass.size(0)\n",
    "        perm_new = torch.zeros_like(perm).to(self.device)\n",
    "        # print('perm:',perm)\n",
    "        # print(perm_new.shape)\n",
    "        perm = xyz.clone()\n",
    "        # print(\"idx:\",idx)\n",
    "        for i in range(B):\n",
    "            # print('i:',i)\n",
    "            perm_new[i] = perm[i][ass[i]]\n",
    "            # print('perm_i',perm[i])\n",
    "            # print('perm_new_i',perm_new[i])\n",
    "\n",
    "        # print('perm_new',perm_new)\n",
    "\n",
    "        return ass,perm_new,dist_mat\n",
    "\n",
    "        # print(\"Done with compute optimal assignment (Heuristic-1)\")\n",
    "        # print(ass.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(z, dist_type='l2'):\n",
    "    '''Return distance matrix between vectors'''\n",
    "    with torch.no_grad():\n",
    "        diff = z.unsqueeze(1) - z.unsqueeze(0)\n",
    "        if dist_type[:2] == 'l2':\n",
    "            A_dist = (diff**2).sum(-1)\n",
    "            if dist_type == 'l2':\n",
    "                A_dist = torch.sqrt(A_dist)\n",
    "            elif dist_type == 'l22':\n",
    "                pass\n",
    "        elif dist_type == 'l1':\n",
    "            A_dist = diff.abs().sum(-1)\n",
    "        elif dist_type == 'linf':\n",
    "            A_dist = diff.abs().max(-1)[0]\n",
    "        else:\n",
    "            return None\n",
    "    return A_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[1,0,0],[0,1,0], [0,0,1]])\n",
    "\n",
    "# distance(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from match import get_onehot_matrix, mix_input\n",
    "from math import ceil\n",
    "\n",
    "import importlib\n",
    "import match\n",
    "importlib.reload(match)\n",
    "from match import get_onehot_matrix, mix_input\n",
    "\n",
    "importlib.reload(gco)\n",
    "# from match import get_onehot_matrix, mix_input\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def mixup_process(out, target_reweighted, args=None, sc=None, A_dist=None):\n",
    "    m_block_num = args.m_block_num\n",
    "    m_part = args.m_part\n",
    "\n",
    "    # batch_size = out.shape[0]\n",
    "    # width = out.shape[-1]\n",
    "\n",
    "    if A_dist is None:\n",
    "        A_dist = torch.eye(batch_size, device=out.device)\n",
    "\n",
    "    if m_block_num == -1:\n",
    "        m_block_num = 2**np.random.randint(1, 5)\n",
    "\n",
    "    \n",
    "    # block_size = width // m_block_num\n",
    "    block_size = 8\n",
    "    # print(\"block size:\",block_size)\n",
    "    # print(\"sc:\",sc.shape) # 8,1024\n",
    "    # sc = sc.unsqueeze(1)\n",
    "    # sc = F.avg_pool1d(sc, block_size)\n",
    "\n",
    "\n",
    "    out_list = []\n",
    "    target_list = []\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sc_part = sc\n",
    "        A_dist_part = A_dist\n",
    "\n",
    "        n_input = sc.shape[0]\n",
    "        # print(\"n_input\", n_input)\n",
    "        # print(\"scpart rehspae\", sc_part.reshape(n_input, -1).shape)\n",
    "        # print(\"sc_part reshape sum\", sc_part.sum(1).shape)\n",
    "        # print(\"total shape\", sc_part.reshape(n_input, -1).sum(1).reshape(n_input, 1, 1).shape)\n",
    "\n",
    "        ## ORIGINAL CODE\n",
    "        # sc_norm = sc_part / sc_part.reshape(n_input, -1).sum(1).reshape(n_input, 1, 1)\n",
    "\n",
    "        ## NEW CODE\n",
    "        # print(\"sc part shape\", sc_part.shape)\n",
    "        # print(\"sc part sum\", sc_part.sum(1).shape)\n",
    "        # sc_norm = sc_part / sc_part.sum(1).reshape(n_input, 1, 1)\n",
    "        sc_norm = sc/torch.sum(sc, dim=1).view(-1,1)\n",
    "        # print(\"sc_norm\", sc_norm.shape)\n",
    "        cost_matrix = -sc_norm\n",
    "        # print(cost_matrix.shape)\n",
    "\n",
    "        A_base = torch.eye(n_input, device=out.device)\n",
    "        A_dist_part = A_dist_part / torch.sum(A_dist_part) * n_input\n",
    "        A = (1 - args.m_omega) * A_base + args.m_omega * A_dist_part\n",
    "        # print(\"A:\",A)\n",
    "\n",
    "        # print(\"new A shape:\",A.shape)\n",
    "        # Return a batch(partitioned) of mixup labeling\n",
    "        # mask_onehot = get_onehot_matrix(cost_matrix.detach(),\n",
    "        #                                 A,\n",
    "        #                                 n_output=n_input,\n",
    "        #                                 beta=args.m_beta,\n",
    "        #                                 gamma=args.m_gamma,\n",
    "        #                                 eta=args.m_eta,\n",
    "        #                                 mixup_alpha=args.mixup_alpha,\n",
    "        #                                 thres=args.m_thres,\n",
    "        #                                 thres_type=args.m_thres_type,\n",
    "        #                                 set_resolve=args.set_resolve,\n",
    "        #                                 niter=args.m_niter,\n",
    "        #                                 device='cuda')\n",
    "\n",
    "\n",
    "        ###### PAY ATTENTION HERE to N OUTPUT - IT'S THE NUMBER OF CLASSES BEING MIXED\n",
    "        mask_onehot = get_onehot_matrix(cost_matrix.detach(),\n",
    "                                        A,\n",
    "                                        n_output=3,\n",
    "                                        beta=args.m_beta,\n",
    "                                        gamma=args.m_gamma,\n",
    "                                        eta=args.m_eta,\n",
    "                                        mixup_alpha=args.mixup_alpha,\n",
    "                                        thres=args.m_thres,\n",
    "                                        thres_type=args.m_thres_type,\n",
    "                                        set_resolve=args.set_resolve,\n",
    "                                        niter=args.m_niter,\n",
    "                                        device='cuda')\n",
    "        \n",
    "    # print('mask onehot shape:',mask_onehot.shape)\n",
    "    # print(mask_onehot)\n",
    "    # Generate image and corrsponding soft target\n",
    "    output_part, target_part = mix_input(mask_onehot, out,\n",
    "                                             target_reweighted)\n",
    "\n",
    "    out_list = output_part\n",
    "    # print(out_list)\n",
    "    target_list = target_part\n",
    "    # print(target_list)\n",
    "    # out_list.append(output_part)\n",
    "    # target_list.append(target_part)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Partition a batch\n",
    "    # for i in range(ceil(batch_size / m_part)):\n",
    "    #     with torch.no_grad():\n",
    "    #         sc_part = sc[i * m_part:(i + 1) * m_part]\n",
    "    #         A_dist_part = A_dist[i * m_part:(i + 1) * m_part, i * m_part:(i + 1) * m_part]\n",
    "\n",
    "    #         n_input = sc_part.shape[0]\n",
    "    #         sc_norm = sc_part / sc_part.reshape(n_input, -1).sum(1).reshape(n_input, 1, 1)\n",
    "    #         cost_matrix = -sc_norm\n",
    "\n",
    "    #         A_base = torch.eye(n_input, device=out.device)\n",
    "    #         A_dist_part = A_dist_part / torch.sum(A_dist_part) * n_input\n",
    "    #         A = (1 - args.m_omega) * A_base + args.m_omega * A_dist_part\n",
    "\n",
    "    #         # Return a batch(partitioned) of mixup labeling\n",
    "    #         mask_onehot = get_onehot_matrix(cost_matrix.detach(),\n",
    "    #                                         A,\n",
    "    #                                         n_output=n_input,\n",
    "    #                                         beta=args.m_beta,\n",
    "    #                                         gamma=args.m_gamma,\n",
    "    #                                         eta=args.m_eta,\n",
    "    #                                         mixup_alpha=args.mixup_alpha,\n",
    "    #                                         thres=args.m_thres,\n",
    "    #                                         thres_type=args.m_thres_type,\n",
    "    #                                         set_resolve=args.set_resolve,\n",
    "    #                                         niter=args.m_niter,\n",
    "    #                                         device='cuda')\n",
    "\n",
    "    #     # Generate image and corrsponding soft target\n",
    "    #     output_part, target_part = mix_input(mask_onehot, out[i * m_part:(i + 1) * m_part],\n",
    "    #                                          target_reweighted[i * m_part:(i + 1) * m_part])\n",
    "\n",
    "    #     out_list.append(output_part)\n",
    "    #     target_list.append(target_part)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     out = torch.cat(out_list, dim=0)\n",
    "    #     target_reweighted = torch.cat(target_list, dim=0)\n",
    "\n",
    "    return output_part, target_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args2 = {'arch': 'preactresnet18', 'batch_size': 32, \n",
    "         'clean_lam': 1.0, 'comix': True, \n",
    "         'data_dir': './data/cifar100/', 'dataset': 'cifar100', \n",
    "         'decay': 0.0001, 'dropout': False, 'epochs': 300, \n",
    "         'evaluate': True, 'gammas': [0.1, 0.1], 'initial_channels': 64, \n",
    "         'labels_per_class': 500, 'learning_rate': 0.2, \n",
    "         'log_off': True, 'm_beta': 0.32, \n",
    "         'm_block_num': 4, 'm_eta': 0.05, \n",
    "         'm_gamma': 1.0, 'm_niter': 4, 'm_omega': 0.001, \n",
    "         'm_part': 20, 'm_thres': 0.83, \n",
    "         'm_thres_type': 'hard', \n",
    "         'mixup_alpha': 2.0, \n",
    "         'momentum': 0.9, 'ngpu': 1, \n",
    "         'parallel': False, 'print_freq': 100, \n",
    "         'resume': './checkpoint/cifar100_preactresnet18_eph300_comixup/checkpoint.pth.tar', \n",
    "         'root_dir': 'experiments', 'schedule': [100, 200], 'seed': 0, \n",
    "         'set_resolve': True, 'start_epoch': 0, 'tag': '', \n",
    "         'use_cuda': True, 'valid_labels_per_class': 0, 'workers': 0}\n",
    "\n",
    "args2 = argparse.Namespace(**args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(args2.m_block_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 8 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/307 [00:15<10:56,  2.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     43\u001b[0m saliency \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mmean(data_var\u001b[39m.\u001b[39mgrad\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 45\u001b[0m assignment,perm_new,align_dist \u001b[39m=\u001b[39m sagemix\u001b[39m.\u001b[39;49mmix(data, label, saliency)\n\u001b[1;32m     46\u001b[0m \u001b[39m# print(perm_new.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# data_var2 = Variable(perm_new.permute(0,2,1), requires_grad=True)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# print(assignment[0])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# print('new_permutation',perm_new.shape)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m target_reweighted \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(label, num_classes\u001b[39m=\u001b[39mnum_class)\u001b[39m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mSageMix.mix\u001b[0;34m(self, xyz, label, saliency)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# print(perm.shape)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m perm  \u001b[39m=\u001b[39m perm\u001b[39m.\u001b[39mreshape(perm\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m1024\u001b[39m,\u001b[39m1024\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m dist, ass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEMD(xyz, perm, \u001b[39m0.005\u001b[39;49m, \u001b[39m500\u001b[39;49m) \u001b[39m# mapping\u001b[39;00m\n\u001b[1;32m     40\u001b[0m      \u001b[39m# 32,1024\u001b[39;00m\n\u001b[1;32m     41\u001b[0m dist_mat[idx] \u001b[39m=\u001b[39m dist\n",
      "File \u001b[0;32m/data/cb/shuvom/anaconda3/envs/mixit/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/cb/shuvom/multimixup/MultiPointMixup/emd_/emd_module.py:79\u001b[0m, in \u001b[0;36memdModule.forward\u001b[0;34m(self, input1, input2, eps, iters)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input1, input2, eps, iters):\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m emdFunction\u001b[39m.\u001b[39;49mapply(input1, input2, eps, iters)\n",
      "File \u001b[0;32m/data/cb/shuvom/anaconda3/envs/mixit/lib/python3.8/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/data/cb/shuvom/multimixup/MultiPointMixup/emd_/emd_module.py:58\u001b[0m, in \u001b[0;36memdFunction.forward\u001b[0;34m(ctx, xyz1, xyz2, eps, iters)\u001b[0m\n\u001b[1;32m     55\u001b[0m unass_cnt_sum \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m512\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     56\u001b[0m cnt_tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m512\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m---> 58\u001b[0m emd\u001b[39m.\u001b[39;49mforward(xyz1, xyz2, dist, assignment, price, assignment_inv, bid, bid_increments, max_increments, unass_idx, unass_cnt, unass_cnt_sum, cnt_tmp, max_idx, eps, iters)\n\u001b[1;32m     60\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(xyz1, xyz2, assignment)\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m dist, assignment\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = PointNet(args, num_class).to(device)\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "opt = optim.SGD(model.parameters(), lr=args.lr*100, momentum=args.momentum, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=args.lr)\n",
    "    \n",
    "\n",
    "sagemix = SageMix(args, device, num_class)\n",
    "criterion = cal_loss_mix\n",
    "\n",
    "\n",
    "best_test_acc = 0\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    ####################\n",
    "    # Train\n",
    "    ####################\n",
    "    train_loss = 0.0\n",
    "    count = 0.0\n",
    "    model.train()\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        # print(data)\n",
    "        batch_size = data.size()[0]\n",
    "        \n",
    "        ####################\n",
    "        # generate augmented sample\n",
    "        ####################\n",
    "        model.eval()\n",
    "        # print(data.permute(0,2,1).shape)\n",
    "        data_var = Variable(data.permute(0,2,1), requires_grad=True)\n",
    "        logits = model(data_var)\n",
    "        loss = cal_loss(logits, label, smoothing=False)\n",
    "        loss.backward()\n",
    "        opt.zero_grad()\n",
    "        saliency = torch.sqrt(torch.mean(data_var.grad**2,1))\n",
    "        \n",
    "        assignment,perm_new,align_dist = sagemix.mix(data, label, saliency)\n",
    "        # print(perm_new.shape)\n",
    "        # data_var2 = Variable(perm_new.permute(0,2,1), requires_grad=True)\n",
    "        \n",
    "        # sc = torch.sqrt(torch.mean(data_var2.grad**2,1))\n",
    "        # print('assignment:',assignment.shape)\n",
    "        # print(assignment[0])\n",
    "        # print('new_permutation',perm_new.shape)\n",
    "        \n",
    "        target_reweighted = F.one_hot(label, num_classes=num_class).float()\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # print(saliency.shape)\n",
    "            sc = saliency.unsqueeze(1)\n",
    "            # print(\"sc:\",sc.shape)\n",
    "            z = F.avg_pool1d(sc, kernel_size=8, stride=1)\n",
    "            # print(\"z:\",z.shape)\n",
    "            z_reshape = z.reshape(args.batch_size, -1)\n",
    "            # print(\"z_reshape:\",z_reshape.shape)\n",
    "            z_idx_1d = torch.argmax(z_reshape, dim=1)\n",
    "            z_idx_2d = torch.zeros((args.batch_size, 2), device=z.device)\n",
    "            z_idx_2d[:, 0] = z_idx_1d // z.shape[-1]\n",
    "            z_idx_2d[:, 1] = z_idx_1d % z.shape[-1]\n",
    "            A_dist = distance(z_idx_2d, dist_type='l1')\n",
    "            # print(\"A_dist:\",A_dist.shape)\n",
    "            # print(A_dist)\n",
    "\n",
    "        # print(A_dist)\n",
    "        # print(\"perm_new\", perm_new.shape)\n",
    "        # print(\"target_reweighted\", target_reweighted.shape)\n",
    "        # print(args2)\n",
    "        # print(\"sc\", sc.shape)\n",
    "        # print(\"A_dist\", A_dist.shape)\n",
    "        out, target_reweighted = mixup_process(perm_new,\n",
    "                                                target_reweighted,\n",
    "                                                args=args2,\n",
    "                                                sc=saliency,\n",
    "                                                A_dist=A_dist)\n",
    "        # print(out, target_reweighted)\n",
    "        # print(out.shape)\n",
    "        # break\n",
    "        model.train()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        logits = model(out.permute(0,2,1))\n",
    "        loss = criterion(logits, target_reweighted)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        count += batch_size\n",
    "        train_loss += loss.item() * batch_size\n",
    "        \n",
    "    scheduler.step()\n",
    "    outstr = 'Train %d, loss: %.6f' % (epoch, train_loss*1.0/count)\n",
    "    print(outstr)\n",
    "    # io.cprint(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:02<00:00, 73.03it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test 2, loss: 2.926748, test acc: 0.341977, test avg acc: 0.211000, best test acc: 0.341977'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "count = 0.0\n",
    "model.eval()\n",
    "test_pred = []\n",
    "test_true = []\n",
    "for data, label in tqdm(test_loader):\n",
    "    data, label = data.to(device), label.to(device).squeeze()\n",
    "    data = data.permute(0, 2, 1)\n",
    "    batch_size = data.size()[0]\n",
    "    logits = model(data)\n",
    "    loss = cal_loss(logits, label)\n",
    "    preds = logits.max(dim=1)[1]\n",
    "    count += batch_size\n",
    "    test_loss += loss.item() * batch_size\n",
    "    test_true.append(label.cpu().numpy())\n",
    "    test_pred.append(preds.detach().cpu().numpy())\n",
    "test_true = np.concatenate(test_true)\n",
    "test_pred = np.concatenate(test_pred)\n",
    "test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "if test_acc >= best_test_acc:\n",
    "    best_test_acc = test_acc\n",
    "#     torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % args.exp_name)\n",
    "outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f, best test acc: %.6f' % (epoch,\n",
    "                                                                        test_loss*1.0/count,\n",
    "                                                                        test_acc,\n",
    "                                                                        avg_per_class_acc,\n",
    "                                                                        best_test_acc)\n",
    "# io.cprint(outstr)\n",
    "outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "\u001b[1;33m[Open3D WARNING] GLFW Error: X11: The DISPLAY environment variable is missing\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLFW\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] [DrawGeometries] Failed creating OpenGL window.\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "# !pip install open3dfind_alignment_and_mapping\n",
    "# import open3d as o3d\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming you have \"points\" as your numpy array containing the points you want to plot\n",
    "# points = np.random.rand(100, 3)\n",
    "\n",
    "# point_cloud = o3d.geometry.PointCloud()\n",
    "# point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "# o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "points = out.squeeze().cpu().numpy()\n",
    "\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "o3d.io.write_point_cloud(\"my_point_cloud.pcd\", point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out.squeeze().cpu().numpy()\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "for i in range(3):\n",
    "    points = data[i].squeeze().cpu().numpy()\n",
    "\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "    o3d.io.write_point_cloud(\"clouds/my_point_cloud_input_{}.pcd\".format(i), point_cloud)\n",
    "\n",
    "points = out.squeeze().cpu().numpy()\n",
    "\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "o3d.io.write_point_cloud(\"clouds/my_point_cloud_output.pcd\", point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_reweighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b486c1e81d32408da1fc5257421bb419818dd94b97c55691de411a2d2d107697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
